	*************************************************
	*                                               *
	*          ONE-View report generation           *
	*                                               *
	*************************************************

Info: Experiment configuration summary is available adding -dbg=1 in command line
Info: 
Info: START THE APPLICATION PROFILING
Info: STOP THE APPLICATION PROFILING
Info: 
Info: START FUNCTIONS AND LOOPS ANALYSIS ...
Info: STOP FUNCTIONS AND LOOPS ANALYSIS ...
Info: 
Info: START THE REPORT GENERATION


+==================================================================================================+
+                                           1  -  GLOBAL                                           +
+==================================================================================================+


+--------------------------------------------------------------------------------------------------+
+                                    1.1  -  Experiment Summary                                    +
+--------------------------------------------------------------------------------------------------+

  Application:			./s13_01
  Timestamp:			2021-03-10 16:19:02
  Experiment Type:		Sequential
  Machine:			pop-os
  Architecture:			x86_64
  Micro Architecture:		KABY_LAKE
  Model Name:			Intel(R) Core(TM) i5-7300HQ CPU @ 2.50GHz
  Cache Size:			6144 KB
  Number of Cores:		4
  OS Version:			Linux 5.8.0-7625-generic #26~1603126178~20.10~210fe73-Ubuntu SMP Tue Oct 20 01:12:15 UTC 
  Compilation Options:		
  Number of processes observed:	1
  Number of threads observed:	1
  MAQAO version:		2.13.0
  MAQAO build:			0a2b44fc1142f4c5c3e2b206c19b6c7b43c1d709::20210226-115937




+--------------------------------------------------------------------------------------------------+
+                                      1.2  -  Global Metrics                                      +
+--------------------------------------------------------------------------------------------------+

  Total Time:				3.92 s
  Time spent in loops:			100 %
  Time spent in innermost loops:	100 %
  Compilation Options:			Not Available
  Perfect Flow Complexity:		6.66
  Array Access Efficiency:		95.83 %
  If No Scalar Integer:
      Potential Speedup:		1.25
      Nb Loops to get 80%:		1
  If FP Vectorized:
      Potential Speedup:		1.16
      Nb Loops to get 80%:		1
  If Fully Vectorized:
      Potential Speedup:		6.66
      Nb Loops to get 80%:		1
  Perfect OpenMP + MPI + Pthread:	1.00




+--------------------------------------------------------------------------------------------------+
+                                    1.3  -  Potential Speedups                                    +
+--------------------------------------------------------------------------------------------------+

  If No Scalar Integer:
      Number of loops   | 1      | 
      Cumulated Speedup | 1.2500 | 
  Top 5 loops:
    s13 - 01 - 7:	1.25

  If FP Vectorized:
      Number of loops   | 1      | 
      Cumulated Speedup | 1.1644 | 
  Top 5 loops:
    s13 - 01 - 7:	1.1644

  If Fully Vectorized:
      Number of loops   | 1      | 
      Cumulated Speedup | 6.6579 | 
  Top 5 loops:
    s13 - 01 - 7:	6.6579



+==================================================================================================+
+                                        2  -  APPLICATION                                         +
+==================================================================================================+


+--------------------------------------------------------------------------------------------------+
+                                      2.1  -  Categorization                                      +
+--------------------------------------------------------------------------------------------------+

   Category | System | Binary | Math  | Pthread | MPI  | OMP  | Memory | IO  | String | Others |
  ----------+--------+--------+-------+---------+------+------+--------+-----+--------+--------+
   Time (%) | 0      | 100    | 0     | 0       | 0    | 0    | 0      | 0   | 0      | 0      |




+--------------------------------------------------------------------------------------------------+
+                                 2.2  -  Function Based Profiling                                 +
+--------------------------------------------------------------------------------------------------+

   Buckets               | Nb Functions          | Coverage              | Cumulated Coverage    |
  -----------------------+-----------------------+-----------------------+-----------------------+
   > 8%                  | 1                     | 100                   | 100                   |
   4% to 8%              | 0                     | 0                     | 100                   |
   2% to 4%              | 0                     | 0                     | 100                   |
   1% to 2%              | 0                     | 0                     | 100                   |
   0.5% to 1%            | 0                     | 0                     | 100                   |
   0.25% to 0.5%         | 0                     | 0                     | 100                   |
   0.125% to 0.25%       | 0                     | 0                     | 100                   |
   < 0.125%              | 0                     | 0                     | 100                   |




+--------------------------------------------------------------------------------------------------+
+                                   2.3  -  Loop Based Profiling                                   +
+--------------------------------------------------------------------------------------------------+

   Buckets               | Nb Loops              | Coverage              | Cumulated Coverage    |
  -----------------------+-----------------------+-----------------------+-----------------------+
   > 8%                  | 1                     | 100                   | 100                   |
   4% to 8%              | 0                     | 0                     | 100                   |
   2% to 4%              | 0                     | 0                     | 100                   |
   1% to 2%              | 0                     | 0                     | 100                   |
   0.5% to 1%            | 0                     | 0                     | 100                   |
   0.25% to 0.5%         | 0                     | 0                     | 100                   |
   0.125% to 0.25%       | 0                     | 0                     | 100                   |
   < 0.125%              | 0                     | 0                     | 100                   |


+==================================================================================================+
+                                         3  -  FUNCTIONS                                          +
+==================================================================================================+


+--------------------------------------------------------------------------------------------------+
+                                     3.1  -  Top 10 Functions                                     +
+--------------------------------------------------------------------------------------------------+

   Function                                     | Module          | Coverage (%) | Time (s)   |
  ----------------------------------------------+-----------------+--------------+------------+
   s13                                          | s13_01          | 100          | 3.93       |


+==================================================================================================+
+                                           4  -  LOOPS                                            +
+==================================================================================================+


+--------------------------------------------------------------------------------------------------+
+                                       4.1  -  Top 10 Loops                                       +
+--------------------------------------------------------------------------------------------------+

   Loop Id    | Module          | Source Location                              | Coverage (%) |
  ------------+-----------------+----------------------------------------------+--------------+
   7          | s13_01          |                                              | 100          |





+==================================================================================================+
+                                            5  -  CQA                                             +
+==================================================================================================+


+--------------------------------------------------------------------------------------------------+
+                                          5.1  -  Loops                                           +
+--------------------------------------------------------------------------------------------------+





      5.1.1  -  Loop 7 from s13_01
  ========================================================================================


This loop has 4 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX



      5.1.1.1  -  Path 1
  ----------------------------------------------------------------------------------------

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

      5.1.1.1.1  -  Vectorization
  ----------------------------------------------------------------------------------------

Your loop is not vectorized.
Only 18% of vector register length is used (average across all SSE/AVX instructions).
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.19 cycles (6.67x speedup).

Details
All SSE/AVX instructions are used in scalar version (process only one data element in vector registers).
Since your execution units are vector units, only a vectorized loop can use their full power.


Workaround
 - Try another compiler or update/tune your current one
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA)



      5.1.1.1.2  -  Execution units bottlenecks
  ----------------------------------------------------------------------------------------

Found no such bottlenecks but see expert reports for more complex bottlenecks.



No data for this section



      5.1.1.1.3  -  Type of elements and instruction set
  ----------------------------------------------------------------------------------------

No instructions are processing arithmetic or math operations on FP elements. This loop is probably writing/copying data or processing integer elements.


      5.1.1.1.4  -  Matching between your loop (in the source code) and the binary loop
  ----------------------------------------------------------------------------------------

The binary loop does not contain any FP arithmetical operations.
The binary loop does not load or store any data.




      5.1.1.2  -  Path 2
  ----------------------------------------------------------------------------------------

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

      5.1.1.2.1  -  Vectorization
  ----------------------------------------------------------------------------------------

Your loop is not vectorized.
Only 16% of vector register length is used (average across all SSE/AVX instructions).
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.25 cycles (7.00x speedup).

Details
All SSE/AVX instructions are used in scalar version (process only one data element in vector registers).
Since your execution units are vector units, only a vectorized loop can use their full power.


Workaround
 - Try another compiler or update/tune your current one
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA)



      5.1.1.2.2  -  Execution units bottlenecks
  ----------------------------------------------------------------------------------------

Found no such bottlenecks but see expert reports for more complex bottlenecks.



No data for this section



      5.1.1.2.3  -  Type of elements and instruction set
  ----------------------------------------------------------------------------------------

No instructions are processing arithmetic or math operations on FP elements. This loop is probably writing/copying data or processing integer elements.


      5.1.1.2.4  -  Matching between your loop (in the source code) and the binary loop
  ----------------------------------------------------------------------------------------

The binary loop does not contain any FP arithmetical operations.
The binary loop does not load or store any data.




      5.1.1.3  -  Path 3
  ----------------------------------------------------------------------------------------

0% of peak computational performance is used (0.22 out of 32.00 FLOP per cycle (GFLOPS @ 1GHz))

      5.1.1.3.1  -  Code clean check
  ----------------------------------------------------------------------------------------

Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.50 to 3.25 cycles (1.38x speedup).

Workaround
 - Try to reorganize arrays of structures to structures of arrays
 - Consider to permute loops (see vectorization gain report)



      5.1.1.3.2  -  Vectorization
  ----------------------------------------------------------------------------------------

Your loop is probably not vectorized.
Only 17% of vector register length is used (average across all SSE/AVX instructions).
By vectorizing your loop, you can lower the cost of an iteration from 4.50 to 0.75 cycles (6.00x speedup).

Details
Store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers).
Since your execution units are vector units, only a vectorized loop can use their full power.


Workaround
 - Try another compiler or update/tune your current one
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA)



      5.1.1.3.3  -  Execution units bottlenecks
  ----------------------------------------------------------------------------------------

Found no such bottlenecks but see expert reports for more complex bottlenecks.



No data for this section



      5.1.1.3.4  -  Complex instructions
  ----------------------------------------------------------------------------------------

Detected COMPLEX INSTRUCTIONS.


Details
These instructions generate more than one micro-operation and only one of them can be decoded during a cycle and the extra micro-operations increase pressure on execution units.
 - CVTSS2SD: 1 occurrences


Workaround
Pass to your compiler a micro-architecture specialization option:
 - Please read your compiler manual



      5.1.1.3.5  -  Slow data structures access
  ----------------------------------------------------------------------------------------

Detected data structures (typically arrays) that cannot be efficiently read/written

Details
 - Constant non-unit stride: 1 occurrence(s)
Non-unit stride (uncontiguous) accesses are not efficiently using data caches


Workaround
 - Try to reorganize arrays of structures to structures of arrays
 - Consider to permute loops (see vectorization gain report)



      5.1.1.3.6  -  Conversion instructions
  ----------------------------------------------------------------------------------------

Detected expensive conversion instructions.

Details
 - CVTSS2SD (FP32 to FP64, scalar): 1 occurrences


Workaround
Avoid mixing data with different types. In particular, check if the type of constants is the same as array elements. Use double instead of single precision only when/where needed by numerical stability and avoid mixing precision.


      5.1.1.3.7  -  Type of elements and instruction set
  ----------------------------------------------------------------------------------------

1 SSE or AVX instructions are processing arithmetic or math operations on single precision FP elements in scalar mode (one at a time).
1 SSE or AVX instructions are processing arithmetic or math operations on double precision FP elements in scalar mode (one at a time).



      5.1.1.3.8  -  Matching between your loop (in the source code) and the binary loop
  ----------------------------------------------------------------------------------------

The binary loop is composed of 1 FP arithmetical operations:
 - 1: divide
The binary loop is loading 8 bytes (2 single precision FP elements).
The binary loop is storing 8 bytes (2 single precision FP elements).


      5.1.1.3.9  -  Arithmetic intensity
  ----------------------------------------------------------------------------------------

Arithmetic intensity is 0.06 FP operations per loaded or stored byte.




      5.1.1.4  -  Path 4
  ----------------------------------------------------------------------------------------

0% of peak computational performance is used (0.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

      5.1.1.4.1  -  Code clean check
  ----------------------------------------------------------------------------------------

Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.75 to 2.75 cycles (1.36x speedup).

Workaround
 - Try to reorganize arrays of structures to structures of arrays
 - Consider to permute loops (see vectorization gain report)



      5.1.1.4.2  -  Vectorization
  ----------------------------------------------------------------------------------------

Your loop is probably not vectorized.
Only 18% of vector register length is used (average across all SSE/AVX instructions).
By vectorizing your loop, you can lower the cost of an iteration from 3.75 to 0.50 cycles (7.47x speedup).

Details
Store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers).
Since your execution units are vector units, only a vectorized loop can use their full power.


Workaround
 - Try another compiler or update/tune your current one
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA)



      5.1.1.4.3  -  Execution units bottlenecks
  ----------------------------------------------------------------------------------------

Found no such bottlenecks but see expert reports for more complex bottlenecks.



No data for this section



      5.1.1.4.4  -  Complex instructions
  ----------------------------------------------------------------------------------------

Detected COMPLEX INSTRUCTIONS.


Details
These instructions generate more than one micro-operation and only one of them can be decoded during a cycle and the extra micro-operations increase pressure on execution units.
 - CVTSS2SD: 1 occurrences


Workaround
Pass to your compiler a micro-architecture specialization option:
 - Please read your compiler manual



      5.1.1.4.5  -  Conversion instructions
  ----------------------------------------------------------------------------------------

Detected expensive conversion instructions.

Details
 - CVTSS2SD (FP32 to FP64, scalar): 1 occurrences


Workaround
Avoid mixing data with different types. In particular, check if the type of constants is the same as array elements. Use double instead of single precision only when/where needed by numerical stability and avoid mixing precision.


      5.1.1.4.6  -  Type of elements and instruction set
  ----------------------------------------------------------------------------------------

1 SSE or AVX instructions are processing arithmetic or math operations on double precision FP elements in scalar mode (one at a time).



      5.1.1.4.7  -  Matching between your loop (in the source code) and the binary loop
  ----------------------------------------------------------------------------------------

The binary loop does not contain any FP arithmetical operations.
The binary loop is loading 4 bytes (0 double precision FP elements).
The binary loop is storing 4 bytes (0 double precision FP elements).





Info: STOP THE REPORT GENERATION
Info: 
