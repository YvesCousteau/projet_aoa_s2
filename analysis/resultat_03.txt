	*************************************************
	*                                               *
	*          ONE-View report generation           *
	*                                               *
	*************************************************

Info: Experiment configuration summary is available adding -dbg=1 in command line
Info: 
Info: START THE APPLICATION PROFILING
Info: STOP THE APPLICATION PROFILING
Info: 
Info: START FUNCTIONS AND LOOPS ANALYSIS ...
Info: STOP FUNCTIONS AND LOOPS ANALYSIS ...
Info: 
Info: START THE REPORT GENERATION


+==================================================================================================+
+                                           1  -  GLOBAL                                           +
+==================================================================================================+


+--------------------------------------------------------------------------------------------------+
+                                    1.1  -  Experiment Summary                                    +
+--------------------------------------------------------------------------------------------------+

  Application:			./s13_03
  Timestamp:			2021-03-10 16:19:16
  Experiment Type:		Sequential
  Machine:			pop-os
  Architecture:			x86_64
  Micro Architecture:		KABY_LAKE
  Model Name:			Intel(R) Core(TM) i5-7300HQ CPU @ 2.50GHz
  Cache Size:			6144 KB
  Number of Cores:		4
  OS Version:			Linux 5.8.0-7625-generic #26~1603126178~20.10~210fe73-Ubuntu SMP Tue Oct 20 01:12:15 UTC 
  Compilation Options:		
  Number of processes observed:	1
  Number of threads observed:	1
  MAQAO version:		2.13.0
  MAQAO build:			0a2b44fc1142f4c5c3e2b206c19b6c7b43c1d709::20210226-115937




+--------------------------------------------------------------------------------------------------+
+                                      1.2  -  Global Metrics                                      +
+--------------------------------------------------------------------------------------------------+

  Total Time:				3.92 s
  Time spent in loops:			100 %
  Time spent in innermost loops:	99.87 %
  Compilation Options:			Not Available
  Perfect Flow Complexity:		6.30
  Array Access Efficiency:		95.83 %
  If No Scalar Integer:
      Potential Speedup:		1.23
      Nb Loops to get 80%:		1
  If FP Vectorized:
      Potential Speedup:		1.20
      Nb Loops to get 80%:		1
  If Fully Vectorized:
      Potential Speedup:		6.33
      Nb Loops to get 80%:		1
  Perfect OpenMP + MPI + Pthread:	1.00




+--------------------------------------------------------------------------------------------------+
+                                    1.3  -  Potential Speedups                                    +
+--------------------------------------------------------------------------------------------------+

  If No Scalar Integer:
      Number of loops   | 1      | 2      | 
      Cumulated Speedup | 1.2349 | 1.2349 | 
  Top 5 loops:
    s13 - 03 - 6:	1.2349
    s13 - 03 - 7:	1.2349

  If FP Vectorized:
      Number of loops   | 1      | 2      | 
      Cumulated Speedup | 1.1954 | 1.1954 | 
  Top 5 loops:
    s13 - 03 - 6:	1.1954
    s13 - 03 - 7:	1.1954

  If Fully Vectorized:
      Number of loops   | 1      | 2      | 
      Cumulated Speedup | 6.2959 | 6.3348 | 
  Top 5 loops:
    s13 - 03 - 6:	6.2959
    s13 - 03 - 7:	6.3348



+==================================================================================================+
+                                        2  -  APPLICATION                                         +
+==================================================================================================+


+--------------------------------------------------------------------------------------------------+
+                                      2.1  -  Categorization                                      +
+--------------------------------------------------------------------------------------------------+

   Category | System | Binary | Math  | Pthread | MPI  | OMP  | Memory | IO  | String | Others |
  ----------+--------+--------+-------+---------+------+------+--------+-----+--------+--------+
   Time (%) | 0      | 100    | 0     | 0       | 0    | 0    | 0      | 0   | 0      | 0      |




+--------------------------------------------------------------------------------------------------+
+                                 2.2  -  Function Based Profiling                                 +
+--------------------------------------------------------------------------------------------------+

   Buckets               | Nb Functions          | Coverage              | Cumulated Coverage    |
  -----------------------+-----------------------+-----------------------+-----------------------+
   > 8%                  | 1                     | 100                   | 100                   |
   4% to 8%              | 0                     | 0                     | 100                   |
   2% to 4%              | 0                     | 0                     | 100                   |
   1% to 2%              | 0                     | 0                     | 100                   |
   0.5% to 1%            | 0                     | 0                     | 100                   |
   0.25% to 0.5%         | 0                     | 0                     | 100                   |
   0.125% to 0.25%       | 0                     | 0                     | 100                   |
   < 0.125%              | 0                     | 0                     | 100                   |




+--------------------------------------------------------------------------------------------------+
+                                   2.3  -  Loop Based Profiling                                   +
+--------------------------------------------------------------------------------------------------+

   Buckets               | Nb Loops              | Coverage              | Cumulated Coverage    |
  -----------------------+-----------------------+-----------------------+-----------------------+
   > 8%                  | 1                     | 99.87                 | 99.87                 |
   4% to 8%              | 0                     | 0                     | 99.87                 |
   2% to 4%              | 0                     | 0                     | 99.87                 |
   1% to 2%              | 0                     | 0                     | 99.87                 |
   0.5% to 1%            | 0                     | 0                     | 99.87                 |
   0.25% to 0.5%         | 0                     | 0                     | 99.87                 |
   0.125% to 0.25%       | 0                     | 0                     | 99.87                 |
   < 0.125%              | 0                     | 0                     | 99.87                 |


+==================================================================================================+
+                                         3  -  FUNCTIONS                                          +
+==================================================================================================+


+--------------------------------------------------------------------------------------------------+
+                                     3.1  -  Top 10 Functions                                     +
+--------------------------------------------------------------------------------------------------+

   Function                                     | Module          | Coverage (%) | Time (s)   |
  ----------------------------------------------+-----------------+--------------+------------+
   s13                                          | s13_03          | 100          | 3.92       |


+==================================================================================================+
+                                           4  -  LOOPS                                            +
+==================================================================================================+


+--------------------------------------------------------------------------------------------------+
+                                       4.1  -  Top 10 Loops                                       +
+--------------------------------------------------------------------------------------------------+

   Loop Id    | Module          | Source Location                              | Coverage (%) |
  ------------+-----------------+----------------------------------------------+--------------+
   6          | s13_03          |                                              | 99.87        |
   7          | s13_03          |                                              | 0.13         |





+==================================================================================================+
+                                            5  -  CQA                                             +
+==================================================================================================+


+--------------------------------------------------------------------------------------------------+
+                                          5.1  -  Loops                                           +
+--------------------------------------------------------------------------------------------------+





      5.1.1  -  Loop 6 from s13_03
  ========================================================================================


This loop has 4 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX



      5.1.1.1  -  Path 1
  ----------------------------------------------------------------------------------------

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

      5.1.1.1.1  -  Vectorization
  ----------------------------------------------------------------------------------------

Your loop is not vectorized.
Only 18% of vector register length is used (average across all SSE/AVX instructions).
By vectorizing your loop, you can lower the cost of an iteration from 1.25 to 0.19 cycles (6.67x speedup).

Details
All SSE/AVX instructions are used in scalar version (process only one data element in vector registers).
Since your execution units are vector units, only a vectorized loop can use their full power.


Workaround
 - Try another compiler or update/tune your current one
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA)



      5.1.1.1.2  -  Execution units bottlenecks
  ----------------------------------------------------------------------------------------

Found no such bottlenecks but see expert reports for more complex bottlenecks.



No data for this section



      5.1.1.1.3  -  Type of elements and instruction set
  ----------------------------------------------------------------------------------------

No instructions are processing arithmetic or math operations on FP elements. This loop is probably writing/copying data or processing integer elements.


      5.1.1.1.4  -  Matching between your loop (in the source code) and the binary loop
  ----------------------------------------------------------------------------------------

The binary loop does not contain any FP arithmetical operations.
The binary loop does not load or store any data.




      5.1.1.2  -  Path 2
  ----------------------------------------------------------------------------------------

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

      5.1.1.2.1  -  Vectorization
  ----------------------------------------------------------------------------------------

Your loop is not vectorized.
Only 16% of vector register length is used (average across all SSE/AVX instructions).
By vectorizing your loop, you can lower the cost of an iteration from 1.75 to 0.25 cycles (7.00x speedup).

Details
All SSE/AVX instructions are used in scalar version (process only one data element in vector registers).
Since your execution units are vector units, only a vectorized loop can use their full power.


Workaround
 - Try another compiler or update/tune your current one
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA)



      5.1.1.2.2  -  Execution units bottlenecks
  ----------------------------------------------------------------------------------------

Found no such bottlenecks but see expert reports for more complex bottlenecks.



No data for this section



      5.1.1.2.3  -  Type of elements and instruction set
  ----------------------------------------------------------------------------------------

No instructions are processing arithmetic or math operations on FP elements. This loop is probably writing/copying data or processing integer elements.


      5.1.1.2.4  -  Matching between your loop (in the source code) and the binary loop
  ----------------------------------------------------------------------------------------

The binary loop does not contain any FP arithmetical operations.
The binary loop does not load or store any data.




      5.1.1.3  -  Path 3
  ----------------------------------------------------------------------------------------

0% of peak computational performance is used (0.25 out of 32.00 FLOP per cycle (GFLOPS @ 1GHz))

      5.1.1.3.1  -  Code clean check
  ----------------------------------------------------------------------------------------

Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 4.00 to 3.00 cycles (1.33x speedup).

Workaround
 - Try to reorganize arrays of structures to structures of arrays
 - Consider to permute loops (see vectorization gain report)



      5.1.1.3.2  -  Vectorization
  ----------------------------------------------------------------------------------------

Your loop is probably not vectorized.
Only 18% of vector register length is used (average across all SSE/AVX instructions).
By vectorizing your loop, you can lower the cost of an iteration from 4.00 to 0.75 cycles (5.33x speedup).

Details
Store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers).
Since your execution units are vector units, only a vectorized loop can use their full power.


Workaround
 - Try another compiler or update/tune your current one
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA)



      5.1.1.3.3  -  Execution units bottlenecks
  ----------------------------------------------------------------------------------------

Found no such bottlenecks but see expert reports for more complex bottlenecks.



No data for this section



      5.1.1.3.4  -  Complex instructions
  ----------------------------------------------------------------------------------------

Detected COMPLEX INSTRUCTIONS.


Details
These instructions generate more than one micro-operation and only one of them can be decoded during a cycle and the extra micro-operations increase pressure on execution units.
 - CVTSS2SD: 1 occurrences


Workaround
Pass to your compiler a micro-architecture specialization option:
 - Please read your compiler manual



      5.1.1.3.5  -  Slow data structures access
  ----------------------------------------------------------------------------------------

Detected data structures (typically arrays) that cannot be efficiently read/written

Details
 - Constant non-unit stride: 1 occurrence(s)
Non-unit stride (uncontiguous) accesses are not efficiently using data caches


Workaround
 - Try to reorganize arrays of structures to structures of arrays
 - Consider to permute loops (see vectorization gain report)



      5.1.1.3.6  -  Conversion instructions
  ----------------------------------------------------------------------------------------

Detected expensive conversion instructions.

Details
 - CVTSS2SD (FP32 to FP64, scalar): 1 occurrences


Workaround
Avoid mixing data with different types. In particular, check if the type of constants is the same as array elements. Use double instead of single precision only when/where needed by numerical stability and avoid mixing precision.


      5.1.1.3.7  -  Type of elements and instruction set
  ----------------------------------------------------------------------------------------

1 SSE or AVX instructions are processing arithmetic or math operations on single precision FP elements in scalar mode (one at a time).
1 SSE or AVX instructions are processing arithmetic or math operations on double precision FP elements in scalar mode (one at a time).



      5.1.1.3.8  -  Matching between your loop (in the source code) and the binary loop
  ----------------------------------------------------------------------------------------

The binary loop is composed of 1 FP arithmetical operations:
 - 1: divide
The binary loop is loading 8 bytes (2 single precision FP elements).
The binary loop is storing 8 bytes (2 single precision FP elements).


      5.1.1.3.9  -  Arithmetic intensity
  ----------------------------------------------------------------------------------------

Arithmetic intensity is 0.06 FP operations per loaded or stored byte.




      5.1.1.4  -  Path 4
  ----------------------------------------------------------------------------------------

0% of peak computational performance is used (0.00 out of 16.00 FLOP per cycle (GFLOPS @ 1GHz))

      5.1.1.4.1  -  Code clean check
  ----------------------------------------------------------------------------------------

Detected a slowdown caused by scalar integer instructions (typically used for address computation).
By removing them, you can lower the cost of an iteration from 3.50 to 2.50 cycles (1.40x speedup).

Workaround
 - Try to reorganize arrays of structures to structures of arrays
 - Consider to permute loops (see vectorization gain report)



      5.1.1.4.2  -  Vectorization
  ----------------------------------------------------------------------------------------

Your loop is probably not vectorized.
Only 19% of vector register length is used (average across all SSE/AVX instructions).
By vectorizing your loop, you can lower the cost of an iteration from 3.50 to 0.47 cycles (7.47x speedup).

Details
Store and arithmetical SSE/AVX instructions are used in scalar version (process only one data element in vector registers).
Since your execution units are vector units, only a vectorized loop can use their full power.


Workaround
 - Try another compiler or update/tune your current one
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA)



      5.1.1.4.3  -  Execution units bottlenecks
  ----------------------------------------------------------------------------------------

Found no such bottlenecks but see expert reports for more complex bottlenecks.



No data for this section



      5.1.1.4.4  -  Complex instructions
  ----------------------------------------------------------------------------------------

Detected COMPLEX INSTRUCTIONS.


Details
These instructions generate more than one micro-operation and only one of them can be decoded during a cycle and the extra micro-operations increase pressure on execution units.
 - CVTSS2SD: 1 occurrences


Workaround
Pass to your compiler a micro-architecture specialization option:
 - Please read your compiler manual



      5.1.1.4.5  -  Conversion instructions
  ----------------------------------------------------------------------------------------

Detected expensive conversion instructions.

Details
 - CVTSS2SD (FP32 to FP64, scalar): 1 occurrences


Workaround
Avoid mixing data with different types. In particular, check if the type of constants is the same as array elements. Use double instead of single precision only when/where needed by numerical stability and avoid mixing precision.


      5.1.1.4.6  -  Type of elements and instruction set
  ----------------------------------------------------------------------------------------

1 SSE or AVX instructions are processing arithmetic or math operations on double precision FP elements in scalar mode (one at a time).



      5.1.1.4.7  -  Matching between your loop (in the source code) and the binary loop
  ----------------------------------------------------------------------------------------

The binary loop does not contain any FP arithmetical operations.
The binary loop is loading 4 bytes (0 double precision FP elements).
The binary loop is storing 4 bytes (0 double precision FP elements).







      5.1.2  -  Loop 7 from s13_03
  ========================================================================================


Warnings:
Non-innermost loop: analyzing only self part (ignoring child loops).
This loop has 4 execution paths.

The presence of multiple execution paths is typically the main/first bottleneck.
Try to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):
 - hoisting them (moving them outside the loop)
 - turning them into conditional moves, MIN or MAX



      5.1.2.1  -  Path 1
  ----------------------------------------------------------------------------------------

Warnings:
This path is accessible from 4 CFG paths (including child blocks)

0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))

      5.1.2.1.1  -  Vectorization
  ----------------------------------------------------------------------------------------

Your loop is not vectorized.
4 data elements could be processed at once in vector registers.
By vectorizing your loop, you can lower the cost of an iteration from 1.50 to 0.37 cycles (4.00x speedup).

Details
All SSE/AVX instructions are used in scalar version (process only one data element in vector registers).
Since your execution units are vector units, only a vectorized loop can use their full power.


Workaround
 - Try another compiler or update/tune your current one
 - Remove inter-iterations dependences from your loop and make it unit-stride:
  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly
  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA)



      5.1.2.1.2  -  Execution units bottlenecks
  ----------------------------------------------------------------------------------------

Found no such bottlenecks but see expert reports for more complex bottlenecks.



No data for this section



      5.1.2.1.3  -  Type of elements and instruction set
  ----------------------------------------------------------------------------------------

No instructions are processing arithmetic or math operations on FP elements. This loop is probably writing/copying data or processing integer elements.


      5.1.2.1.4  -  Matching between your loop (in the source code) and the binary loop
  ----------------------------------------------------------------------------------------

The binary loop does not contain any FP arithmetical operations.
The binary loop does not load or store any data.





Info: STOP THE REPORT GENERATION
Info: 
